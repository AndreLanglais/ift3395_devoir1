\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage{amsmath}

\title{Devoir \#1 - IFT3395/6390}
\author{Julien Allard et André Langlais}
\date{12 Octobre 2017}

\begin{document}
	
	\maketitle
	\newpage
	\section{Petit exercice de probabilités}
	\indent Soit les variables aléatoires suivantes: \\	
	 		X: Femme atteinte du cancer \\ 		 			
	 		Y: Test de détection est positif \\
	 		
	 Formule de Bayes:
	 	
	 \begin{equation}
		P(X|Y)= \frac{P(X)*P(Y|X)}{P(Y)}
	 \end{equation}
	 	
	 	
	 Les données de la question nous permettent de trouver les probabilités suivantes:
	 \begin{equation}
	 	\begin{split}
	 	& P(X) = 0.01 \\
	 	& P(Y|X)  = 0.8 \\
	 	\end{split}
	 \end{equation}
	 
	 Il nous manque à trouver {P(Y)} qui peut être calculer par la formule suivante:
	 \begin{equation}
	 P(Y) = P(Y|X) + P(Y|X')
	 \end{equation}
	 
	 où X' répresente l'évènement complémentaire de X
	 
	 \begin{equation}
	 P(Y)= 0.01*0.80 + 0.99*0.096 = 0.10304
	 \end{equation}
	 
	 Nous avons maintenant toutes les informations requises pour appliquer le théorème de Bayes
	 
	 	 \begin{equation}
	 	 P(X|Y)= \frac{0.01*0.80}{0.10304} \approx 0.0774
	 	 \end{equation}
	 
	 La bonne réponse est donc 6. soit moins que 10\%. 	 
	 Il peut paraître surprenant que le pourcentage soit si bas, mais on peut vérifier visuellement pourquoi on obtient un tel pourcentage
	
	
	\section{Estimation de densité : 
		\\paramétrique Gaussienne, v.s. fenêtres de Parzen}
	\subsection{Gaussienne isotropique}
	
	\subsubsection*{(a)}
	Les paramètres sont la moyenne $\mu$ de dimension d et la variance $\sigma$ de dimension 1 \\
	
	
	\subsubsection*{(b)}
	\begin{equation}
	\begin{split}
		n &= |D| \\
		\mu_{MaxVraiss} &= \sum_{i}^{n} \frac{x_{i}}{nd} \\
		\sigma_{MaxVraiss}^{2} &= \sum_{i}^{n} \frac{(x_{i}- \mu)^{T}(x_{i} - \mu)}{nd} \\
	\end{split}
	\end{equation}
	
	\subsubsection*{(c)}
	$\mu$ se calcule en $O(nd)$  et $\sigma^{2} $ se calcule en $O(nd)$ aussi\\
	\subsubsection*{(d)}
	
	\begin{equation}
		p_{gauss-isotrop}(x) = 
		\frac{1}{(2\pi)^{\frac{d}{2}}\sigma^d}\exp^{\frac{-1}{2}\frac{\Vert x-\mu\Vert^2}{\sigma^2}}
	\end{equation}
	
	\subsubsection*{(e)}
	Le temps de calcul pour la prédiction d'un point de test $x$ se fait en $O(1)$
	
	\subsection{Fenêtres de Parzen avec noyau Gaussien isotropique}
	
	\subsubsection*{(a)}
	Puisque $\sigma$ est fixé, la phase d'entraînement consiste à garder l'ensemble d'entraînement en mémoire.
	\subsubsection*{(b)}
	\begin{equation}
	p_{mathrmParzen}(x) =
	\frac{1}{n}\sum_{i}^{n}\frac{1}{(2\pi)^{\frac{d}{2}}\sigma^d}\exp^{\frac{-1}{2}\frac{\Vert x-x_i\Vert^2}{\sigma^2}}
	\end{equation}
	\subsubsection*{(c)}
	Le temps de calcul pour la prédiction d'un point de test $x$ se fait en $O(n)$
	
	\subsection{Capacité}
	
	\subsubsection*{(a)}
	Parzen a plus de capacité puisqu'on peut définir des régions de décisions plus précise. Il y a plus de changement rapide lorsque l'on se déplace dans la région totale.
	
	\subsubsection*{(b)}
	Parzen est le modèle qui peut plus facilement être dans un cas de surapprentissage. Cela se produit dans le cas où les données sont plus éparpillées et lorsque le $sigma$ est très petit.
	
	\subsubsection*{(c)}
	Pour la Gaussienne, on veut optimiser les paramètres selon les données. Elles sont optimisables avec la méthode de vraisemblance. Pour Parzen, on ne peut pas optimiser le $sigma$ selon la méthode de vraisemblance. En effet, la variance de la Gaussienne au point $x$ ne se calcule pas puisqu'il n'y a qu'une seule donnée pour cette Gaussienne.
	
	\subsection{Gaussienne diagonale}
	
	\subsubsection*{(a)}
	\begin{equation}
	p_{gausDiag}(x) = \frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{\vert\Sigma\vert}}\exp^{\frac{-1}{2}\Vert x-\mu\Vert^T{\Sigma^{-1}}(x-\mu)}
	\end{equation}
	\subsubsection*{(b)}
	Les composantes sont chacune tirées d'un distribution gaussienne donc elles sont aléatoires. De plus, le fait de tirer une des composantes n'impacte pas les prochaines donc elles sont indépendantes.
	\subsubsection*{(c)}
	Voici l'équation qui permettrait de minimiser le risque empirique selon le maximum de vraisemblance
	\begin{equation}
	\begin{split}
		\prod_{i}^{n} p(x_{i}) = -log(\prod_{i}^{n}\frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{\vert\Sigma\vert}}\exp^{\frac{-1}{2}\Vert x_{i}-\mu\Vert^T{\Sigma^{-1}}(x_{i}-\mu)})
	\end{split}
	\end{equation}
	\subsubsection*{(d)}
	Résolution de l'équation
		\begin{equation}
		\begin{split}
		\prod_{i}^{n} p(x_{i}) &=
		- \sum log(\frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{\vert\Sigma\vert}}\exp^{\frac{-1}{2} (x_{i}-\mu)^T{\Sigma^{-1}}(x_{i}-\mu)}) \\		
		 &= -\sum log(\frac{1}{(2\pi)^{\frac{d}{2}}\sqrt{\vert\Sigma\vert}}) - \frac{1}{2}( x_{i}-\mu)^T{\Sigma^{-1}}(x_{i}-\mu) \\
		 &= -\sum -log(2\pi) - log(\sqrt{\vert\Sigma\vert}) - \frac{1}{2}( x_{i}-\mu)^T{\Sigma^{-1}}(x_{i}-\mu)\\
		 &=\frac{nd}{2} log(2\pi) + n log(\sqrt{\vert\Sigma\vert}) + \sum \frac{1}{2}( x_{i}-\mu)^T{\Sigma^{-1}}(x_{i}-\mu)		 
		\end{split}
		\end{equation}
	
	On dérive par rapport à $\mu$ pour trouver le premier paramètre
		\begin{equation}
		\begin{split}
		\frac{\partial}{\partial \mu}\prod_{i}^{n} p(x_{i}) &= 0 + 0 + \sum 2(\mu - x_{i}) \Sigma^{-1}	 
		\end{split}
		\end{equation}	
		
	On la met égal à zéro
		\begin{equation}
		\begin{split}
		\frac{\partial}{\partial \mu}\prod_{i}^{n} p(x_{i}) &= 0 \\
		\sum 2(\mu - x_{i}) \Sigma^{-1} &= 0 \\
		\mu = \frac{\sum_{i}^{n} x_{i}}{n}
		\end{split}
		\end{equation}		
	
	\subsection{Problème de classification}
		
	\subsubsection*{(a)}
	En se basant sur un classifieur de Bayes et en utilisant comme modèle une Gaussien isotropique, on doit d'abord entraîner les données en estimant $\mu$ et $\sigma^2$ à l'aide de la méthode de vraisemblance.
	Ensuite, on peut déterminer les probabilités à priori pour les différentes classes.
		
	\subsubsection*{(b)}
	\begin{equation}
	\begin{split}
	c &\in \{1, ... , n\} \\
	B_{Gaussisotrop}(x) &= 
	\frac{1}{(2\pi)^{\frac{d}{2}}\sigma^d}\exp^{\frac{-1}{2}\frac{\Vert x-\mu\Vert^2}{\sigma^2}} + \log(P_c = \frac{n_c}{n} \approx P(Y = c))
	\end{split}
	\end{equation}
	
\end{document}\
